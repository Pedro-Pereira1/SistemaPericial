{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-5R0L9vKR3R"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N6_fa_xmKR3R"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import joblib\n",
    "import matplotlib as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import xgboost as xgb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Conv1D, MaxPooling1D, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "import shap\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqAPKYpzKR3S"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eSP2ARbDKR3S"
   },
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_parquet(\"data/cic-collection.parquet\")  # Replace with the correct path to the dataset\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop(['Label','ClassLabel'], axis=1)  # Replace 'target' with the correct column name\n",
    "y = data['ClassLabel']\n",
    "\n",
    "# Encode target if categorical\n",
    "if y.dtype == 'object':\n",
    "    y = pd.factorize(y)[0]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u2eWpRpLKR3S"
   },
   "outputs": [],
   "source": [
    "# Reduce the size of the dataset for faster training\n",
    "X_train = X_train[:10000]\n",
    "y_train = y_train[:10000]\n",
    "X_test = X_test[:1000]\n",
    "y_test = y_test[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqg8kjnqKR3S"
   },
   "source": [
    "# Function to calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gz1PchGSKR3S"
   },
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, training_time, inference_time):\n",
    "    return {\n",
    "        \"Accuracy\": round(accuracy_score(y_true, y_pred), 4),\n",
    "        \"Precision\": round(precision_score(y_true, y_pred, average=\"weighted\"), 4),\n",
    "        \"Recall\": round(recall_score(y_true, y_pred, average=\"weighted\"), 4),\n",
    "        \"F1\": round(f1_score(y_true, y_pred, average=\"weighted\"), 4),\n",
    "        \"Training Time\": round(training_time, 4),\n",
    "        \"Inference Time\": round(inference_time, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9Tt7I-YKR3S"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "# Isto está formatado como código\n",
    "```\n",
    "\n",
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ia8QnpasKR3S"
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter grid\n",
    "#param_grid_rf = {\n",
    "#    'n_estimators': [50, 100, 200, 500],\n",
    "#    'max_depth': [5, 10, 20, None],\n",
    "#    'min_samples_split': [2],\n",
    "#    'min_samples_leaf': [1],\n",
    "#    'max_features': ['sqrt']\n",
    "#}\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200, 500],\n",
    "    'max_depth': [5, 10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# Generate all combinations of parameters\n",
    "all_params = list(itertools.product(\n",
    "    param_grid_rf['n_estimators'],\n",
    "    param_grid_rf['max_depth'],\n",
    "    param_grid_rf['min_samples_split'],\n",
    "    param_grid_rf['min_samples_leaf'],\n",
    "    param_grid_rf['max_features']\n",
    "))\n",
    "\n",
    "# Initialize progress bar\n",
    "progress_bar = tqdm(total=len(all_params), desc=\"Training models\", unit=\"model\")\n",
    "\n",
    "best_score = -1\n",
    "best_params = None\n",
    "cv_results = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for params in all_params:\n",
    "    # Unpack parameters\n",
    "    param_dict = {\n",
    "        'n_estimators': params[0],\n",
    "        'max_depth': params[1],\n",
    "        'min_samples_split': params[2],\n",
    "        'min_samples_leaf': params[3],\n",
    "        'max_features': params[4],\n",
    "    }\n",
    "    \n",
    "    # Update model with current parameters\n",
    "    rf.set_params(**param_dict)\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    scores = cross_val_score(rf, X_train, y_train, cv=5, scoring='f1_macro', n_jobs=-1)\n",
    "    mean_score = scores.mean()\n",
    "    std_score = scores.std()\n",
    "    \n",
    "    # Save results\n",
    "    cv_results.append({**param_dict, 'mean_f1': mean_score, 'std_f1': std_score})\n",
    "    \n",
    "    # Update best parameters if needed\n",
    "    if mean_score > best_score:\n",
    "        best_score = mean_score\n",
    "        best_params = param_dict\n",
    "    \n",
    "    # Update progress bar\n",
    "    progress_bar.update(1)\n",
    "\n",
    "progress_bar.close()\n",
    "\n",
    "training_time_rf = time.time() - start_time\n",
    "\n",
    "# Train final model with best parameters\n",
    "best_rf = RandomForestClassifier(**best_params, random_state=42)\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "start_time = time.time()\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "inference_time_rf = time.time() - start_time\n",
    "\n",
    "metrics_rf = calculate_metrics(y_test, y_pred_rf, training_time_rf, inference_time_rf)\n",
    "print(\"Random Forest Metrics:\", metrics_rf)\n",
    "\n",
    "# Save results and the best model\n",
    "results_rf = pd.DataFrame(cv_results)\n",
    "results_rf.to_csv('gridsearch_rf_results.csv', index=False)\n",
    "print(\"GridSearchCV results saved to 'gridsearch_rf_results.csv'.\")\n",
    "\n",
    "joblib.dump(best_rf, \"best_random_forest_model.pkl\")\n",
    "print(\"Model saved as 'best_random_forest_model.pkl'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x9jy4ilmu6k_"
   },
   "outputs": [],
   "source": [
    "## XAI: SHAP Analysis\n",
    "#explainer = shap.TreeExplainer(best_rf)\n",
    "#shap_values = explainer.shap_values(X_test)\n",
    "#\n",
    "## Plot global feature importance\n",
    "#shap.summary_plot(shap_values, X_test, plot_type=\"bar\", show=False)\n",
    "#plt.savefig(\"shap_feature_importance.png\")\n",
    "#print(\"SHAP global feature importance saved as 'shap_feature_importance.png'.\")\n",
    "#\n",
    "## Identify top 10 important features\n",
    "#feature_importance = best_rf.feature_importances_\n",
    "#important_features = pd.Series(feature_importance, index=X_train.columns).sort_values(ascending=False)\n",
    "#top_features = important_features.head(10)\n",
    "#print(\"Top 10 Features:\\n\", top_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()  # Use MinMaxScaler() if you prefer normalization to [0, 1]\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u60Sb8fpKR3S"
   },
   "source": [
    "# XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oN0ODUBkKR3T"
   },
   "outputs": [],
   "source": [
    "# Define the model and hyperparameter grid\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    use_label_encoder=False, eval_metric='mlogloss', random_state=42, tree_method='gpu_hist'\n",
    ")\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 6, 10, 20],\n",
    "    'learning_rate': [0.01],\n",
    "    'subsample': [0.8],\n",
    "    'colsample_bytree': [0.8],\n",
    "    'gamma': [0.1],\n",
    "}\n",
    "\n",
    "#param_grid_xgb = {\n",
    "#    #'n_estimators': [100, 200, 300],\n",
    "#    #'max_depth': [3, 6, 10, 20],\n",
    "#    #'learning_rate': [0.001, 0.01, 0.05, 0.1, 0.2],\n",
    "#    #'subsample': [0.6, 0.8, 1.0],\n",
    "#    #'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "#    #'gamma': [0, 0.1, 1],\n",
    "#}\n",
    "\n",
    "# Generate all combinations of parameters\n",
    "all_params = list(ParameterGrid(param_grid_xgb))\n",
    "\n",
    "# Initialize progress bar\n",
    "progress_bar = tqdm(total=len(all_params), desc=\"Training XGBoost models\", unit=\"model\")\n",
    "\n",
    "# To store results\n",
    "cv_results = []\n",
    "best_score = -1\n",
    "best_params = None\n",
    "\n",
    "# Start grid search\n",
    "start_time = time.time()\n",
    "for params in all_params:\n",
    "    # Update the model with the current parameters\n",
    "    xgb_model.set_params(**params)\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    scores = cross_val_score(xgb_model, X_train, y_train, cv=5, scoring='f1_weighted', n_jobs=-1)\n",
    "    scores = scores[~np.isnan(scores)]\n",
    "    mean_score = scores.mean()\n",
    "    std_score = scores.std()\n",
    "    \n",
    "    # Save results\n",
    "    cv_results.append({**params, 'mean_f1': mean_score, 'std_f1': std_score})\n",
    "\n",
    "    # Update best parameters\n",
    "    if mean_score > best_score:\n",
    "        best_score = mean_score\n",
    "        best_params = params\n",
    "\n",
    "    # Update progress bar\n",
    "    progress_bar.update(1)\n",
    "\n",
    "progress_bar.close()\n",
    "training_time_xgb = time.time() - start_time\n",
    "\n",
    "# Train the best model on the entire training set\n",
    "best_xgb = xgb.XGBClassifier(\n",
    "    use_label_encoder=False, eval_metric='mlogloss', random_state=42, device='cuda', **best_params\n",
    ")\n",
    "best_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Test set predictions\n",
    "start_time = time.time()\n",
    "y_pred_xgb = best_xgb.predict(X_test)\n",
    "inference_time_xgb = time.time() - start_time\n",
    "\n",
    "# Calculate metrics\n",
    "metrics_xgb = calculate_metrics(y_test, y_pred_xgb, training_time_xgb, inference_time_xgb)\n",
    "print(\"XGBoost Metrics:\", metrics_xgb)\n",
    "\n",
    "# Save results\n",
    "results_xgb = pd.DataFrame(cv_results)\n",
    "results_xgb.to_csv('gridsearch_xgb_results.csv', index=False)\n",
    "print(\"GridSearchCV results saved to 'gridsearch_xgb_results.csv'.\")\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_xgb, \"best_xgboost_model.pkl\")\n",
    "print(\"XGBoost model saved as 'best_xgboost_model.pkl'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lwLj1x-bvL72"
   },
   "outputs": [],
   "source": [
    "## SHAP Analysis\n",
    "#explainer = shap.TreeExplainer(best_xgb)\n",
    "#shap_values = explainer.shap_values(X_test)\n",
    "#\n",
    "## Global Feature Importance\n",
    "#shap.summary_plot(shap_values, X_test, plot_type=\"bar\", show=False)\n",
    "#plt.savefig(\"xgb_shap_feature_importance.png\")\n",
    "#print(\"SHAP global feature importance saved as 'xgb_shap_feature_importance.png'.\")\n",
    "#\n",
    "## Local Explanation for a Single Prediction\n",
    "#sample_index = 0  # Change this index to visualize a specific sample\n",
    "#shap.force_plot(\n",
    "#    explainer.expected_value,\n",
    "#    shap_values[sample_index],\n",
    "#    X_test.iloc[sample_index],\n",
    "#    matplotlib=True,\n",
    "#).savefig(\"xgb_shap_local_explanation.png\")\n",
    "#print(\"SHAP local explanation for a sample saved as 'xgb_shap_local_explanation.png'.\")\n",
    "#\n",
    "## Identify Top 10 Features\n",
    "#feature_importance = best_xgb.feature_importances_\n",
    "#important_features = pd.Series(feature_importance, index=X_train.columns).sort_values(ascending=False)\n",
    "#top_features = important_features.head(10)\n",
    "#print(\"Top 10 Features:\\n\", top_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxQBgxE5KR3T"
   },
   "source": [
    "# KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OP6EZ3cdKR3T"
   },
   "outputs": [],
   "source": [
    "# Define the model and hyperparameter grid\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 15],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'p': [1, 2]\n",
    "}\n",
    "\n",
    "# Generate all combinations of parameters\n",
    "all_params = list(ParameterGrid(param_grid_knn))\n",
    "\n",
    "# Initialize progress bar\n",
    "progress_bar = tqdm(total=len(all_params), desc=\"Training KNN models\", unit=\"model\")\n",
    "\n",
    "# To store results\n",
    "cv_results = []\n",
    "best_score = -1\n",
    "best_params = None\n",
    "\n",
    "# Start grid search\n",
    "start_time = time.time()\n",
    "for params in all_params:\n",
    "    # Update the model with the current parameters\n",
    "    knn.set_params(**params)\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='f1_macro', n_jobs=-1)\n",
    "    mean_score = scores.mean()\n",
    "    std_score = scores.std()\n",
    "    \n",
    "    # Save results\n",
    "    cv_results.append({**params, 'mean_f1': mean_score, 'std_f1': std_score})\n",
    "    \n",
    "    # Update best parameters\n",
    "    if mean_score > best_score:\n",
    "        best_score = mean_score\n",
    "        best_params = params\n",
    "\n",
    "    # Update progress bar\n",
    "    progress_bar.update(1)\n",
    "\n",
    "progress_bar.close()\n",
    "training_time_knn = time.time() - start_time\n",
    "\n",
    "# Train the best model on the entire training set\n",
    "best_knn = KNeighborsClassifier(**best_params)\n",
    "best_knn.fit(X_train, y_train)\n",
    "\n",
    "# Test set predictions\n",
    "start_time = time.time()\n",
    "y_pred_knn = best_knn.predict(X_test)\n",
    "inference_time_knn = time.time() - start_time\n",
    "\n",
    "# Calculate metrics\n",
    "metrics_knn = calculate_metrics(y_test, y_pred_knn, training_time_knn, inference_time_knn)\n",
    "print(\"KNN Metrics:\", metrics_knn)\n",
    "\n",
    "# Save results\n",
    "results_knn = pd.DataFrame(cv_results)\n",
    "results_knn.to_csv('gridsearch_knn_results.csv', index=False)\n",
    "print(\"GridSearchCV results saved to 'gridsearch_knn_results.csv'.\")\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_knn, \"best_knn_model.pkl\")\n",
    "print(\"KNN model saved as 'best_knn_model.pkl'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "t4tXQVSkvaZB",
    "outputId": "3c9384f9-cd28-4923-9943-27270f6091ae"
   },
   "outputs": [],
   "source": [
    "## SHAP Analysis for KNN\n",
    "#explainer = shap.KernelExplainer(grid_knn.best_estimator_.predict_proba, shap.sample(X_train, 100))  # Use a sample of the training data for efficiency\n",
    "#shap_values = explainer.shap_values(X_test, nsamples=100)\n",
    "#\n",
    "## Global Feature Importance\n",
    "#shap.summary_plot(shap_values, X_test, plot_type=\"bar\", show=False)\n",
    "#plt.savefig(\"knn_shap_feature_importance.png\")\n",
    "#print(\"SHAP global feature importance saved as 'knn_shap_feature_importance.png'.\")\n",
    "#\n",
    "## Local Explanation for a Single Prediction\n",
    "#sample_index = 0  # Change this index to visualize a specific sample\n",
    "#shap.force_plot(\n",
    "#    explainer.expected_value,\n",
    "#    shap_values[1][sample_index],  # Assumes binary classification; change index if multi-class\n",
    "#    X_test.iloc[sample_index],\n",
    "#    matplotlib=True,\n",
    "#).savefig(\"knn_shap_local_explanation.png\")\n",
    "#print(\"SHAP local explanation for a sample saved as 'knn_shap_local_explanation.png'.\")\n",
    "#\n",
    "#mean_shap_values = np.abs(shap_values[1]).mean(axis=0)  # Change `1` to the index of the desired class if multi-class\n",
    "#top_features = pd.Series(mean_shap_values, index=X_test.columns).sort_values(ascending=False).head(10)\n",
    "#print(\"Top 10 Features Based on SHAP Values:\\n\", top_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0WPsBkQOKR3T"
   },
   "source": [
    "# CNN + RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n7iMgRsOKR3T"
   },
   "outputs": [],
   "source": [
    "# Function to build the CNN+RNN model\n",
    "def create_cnn_rnn_model(conv_filters=64, lstm_units=64, dense_units=128, dropout_rate=0.5, learning_rate=0.001):\n",
    "    model = Sequential([\n",
    "        Conv1D(conv_filters, 3, activation='relu', input_shape=(X_train_dl.shape[1], 1)),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        LSTM(lstm_units, return_sequences=False),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(dense_units, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(len(np.unique(y_train)), activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Reshape data for CNN+RNN\n",
    "X_train_dl = np.expand_dims(X_train, axis=2)\n",
    "X_test_dl = np.expand_dims(X_test, axis=2)\n",
    "y_train_dl = to_categorical(y_train)\n",
    "y_test_dl = to_categorical(y_test)\n",
    "\n",
    "# Define parameter grid\n",
    "#param_grid_cnn_rnn = {\n",
    "#    'conv_filters': [32, 64, 128],\n",
    "#    'lstm_units': [32, 64, 128],\n",
    "#    'dense_units': [64, 128, 256],\n",
    "#    'dropout_rate': [0.3, 0.5, 0.7],\n",
    "#    'batch_size': [16, 32, 64],\n",
    "#    'epochs': [10, 20],\n",
    "#}\n",
    "\n",
    "param_grid_cnn_rnn = {\n",
    "    'conv_filters': [32, 64, 128],\n",
    "    'lstm_units': [64],\n",
    "    'dense_units': [64],\n",
    "    'dropout_rate': [0.5,],\n",
    "    'batch_size': [16, ],\n",
    "    'epochs': [10],\n",
    "}\n",
    "\n",
    "# Generate all parameter combinations\n",
    "all_params = list(ParameterGrid(param_grid_cnn_rnn))\n",
    "\n",
    "# Initialize progress bar\n",
    "progress_bar = tqdm(total=len(all_params), desc=\"Training CNN+RNN models\", unit=\"model\")\n",
    "\n",
    "# To store results\n",
    "cv_results = []\n",
    "best_score = -1\n",
    "best_params = None\n",
    "\n",
    "# Start grid search\n",
    "start_time = time.time()\n",
    "for params in all_params:\n",
    "    # Create the model with current parameters\n",
    "    model = create_cnn_rnn_model(\n",
    "        conv_filters=params['conv_filters'],\n",
    "        lstm_units=params['lstm_units'],\n",
    "        dense_units=params['dense_units'],\n",
    "        dropout_rate=params['dropout_rate']\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train_dl, y_train_dl,\n",
    "        validation_split=0.2,\n",
    "        batch_size=params['batch_size'],\n",
    "        epochs=params['epochs'],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model\n",
    "    score = model.evaluate(X_test_dl, y_test_dl, verbose=0)\n",
    "    accuracy = score[1]  # Assuming the second value is accuracy\n",
    "    \n",
    "    # Save results\n",
    "    cv_results.append({**params, 'accuracy': accuracy})\n",
    "    \n",
    "    # Update best model parameters\n",
    "    if accuracy > best_score:\n",
    "        best_score = accuracy\n",
    "        best_params = params\n",
    "        best_model = model  # Save the best model\n",
    "\n",
    "    # Update progress bar\n",
    "    progress_bar.update(1)\n",
    "\n",
    "progress_bar.close()\n",
    "training_time_cnn_rnn = time.time() - start_time\n",
    "\n",
    "# Best model inference\n",
    "start_time = time.time()\n",
    "y_pred_cnn_rnn = np.argmax(best_model.predict(X_test_dl), axis=1)\n",
    "inference_time_cnn_rnn = time.time() - start_time\n",
    "\n",
    "# Calculate metrics\n",
    "metrics_cnn_rnn = calculate_metrics(y_test, y_pred_cnn_rnn, training_time_cnn_rnn, inference_time_cnn_rnn)\n",
    "print(\"CNN+RNN Metrics:\", metrics_cnn_rnn)\n",
    "\n",
    "# Save the best model\n",
    "best_model.save(\"best_cnn_rnn_model.h5\")\n",
    "print(\"Best CNN+RNN model saved as 'best_cnn_rnn_model.h5'.\")\n",
    "\n",
    "# Save Grid Search Results\n",
    "results_cnn_rnn = pd.DataFrame(cv_results)\n",
    "results_cnn_rnn.to_csv('gridsearch_cnn_rnn_results.csv', index=False)\n",
    "print(\"GridSearchCV results saved to 'gridsearch_cnn_rnn_results.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ruLwI_3vlpm"
   },
   "outputs": [],
   "source": [
    "## SHAP Analysis for CNN+RNN\n",
    "## Create a SHAP explainer. We use KernelExplainer because the model is a black-box.\n",
    "#explainer = shap.KernelExplainer(best_cnn_rnn_model.model.predict, shap.sample(X_train_dl, 100))  # Use a sample of the training data for efficiency\n",
    "#shap_values = explainer.shap_values(X_test_dl, nsamples=100)\n",
    "#\n",
    "## Global Feature Importance (Bar plot)\n",
    "#shap.summary_plot(shap_values, X_test_dl, plot_type=\"bar\", show=False)\n",
    "#plt.savefig(\"cnn_rnn_shap_feature_importance.png\")\n",
    "#print(\"SHAP global feature importance saved as 'cnn_rnn_shap_feature_importance.png'.\")\n",
    "#\n",
    "## Local Explanation for a Single Prediction (change sample_index for a specific instance)\n",
    "#sample_index = 0  # Change this index to visualize a specific sample\n",
    "#shap.force_plot(\n",
    "#    explainer.expected_value,\n",
    "#    shap_values[1][sample_index],  # Assumes binary classification; change index if multi-class\n",
    "#    X_test_dl[sample_index],\n",
    "#    matplotlib=True,\n",
    "#).savefig(\"cnn_rnn_shap_local_explanation.png\")\n",
    "#print(\"SHAP local explanation for a sample saved as 'cnn_rnn_shap_local_explanation.png'.\")\n",
    "#\n",
    "## Calculate the mean absolute SHAP value for each feature\n",
    "#mean_shap_values = np.abs(shap_values[1]).mean(axis=0)  # Adjust index for multi-class\n",
    "#top_features = pd.Series(mean_shap_values, index=X_test.columns).sort_values(ascending=False).head(10)\n",
    "#print(\"Top 10 Features Based on SHAP Values:\\n\", top_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbZs7at8KR3T"
   },
   "source": [
    "# Save metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2nZzhG44KR3T"
   },
   "outputs": [],
   "source": [
    "all_metrics = pd.DataFrame([metrics_rf, metrics_xgb, metrics_knn, metrics_cnn_rnn],\n",
    "                           index=[\"Random Forest\", \"XGBoost\", \"KNN\", \"CNN+RNN\"])\n",
    "all_metrics.to_csv('model_metrics.csv')\n",
    "print(\"Metrics saved to 'model_metrics.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
